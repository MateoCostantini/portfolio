{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error 9123360941.633387\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def dataset(data):\n",
    "    random.seed(32)\n",
    "    data_ = list(zip( data['idx'], data['forecasted_performance'],data['hs_sleep'], data['hs_study'], data['prev_scores'], data['practice'], data['extracurricular_activities']))\n",
    "    random.shuffle(data_)\n",
    "    idx, y, x1, x2, x3, x4, x5 = zip(*data_)\n",
    "    x1 = list(x1)\n",
    "    x2 = list(x2)\n",
    "    x3 = list(x3)\n",
    "    x4 = list(x4)\n",
    "    x5= list(x5)\n",
    "    y = list(y)\n",
    "    for i in range(len(x5)):\n",
    "        if x5[i] == False:\n",
    "            x5[i] = 0\n",
    "        else:\n",
    "            x5[i] = 1\n",
    "\n",
    "    X = np.zeros((len(x1), 5))\n",
    "    for i in range(len(X)):\n",
    "        X[i][0] = x1[i]\n",
    "        X[i][1] = x2[i]\n",
    "        X[i][2] = x3[i]\n",
    "        X[i][3] = x4[i]\n",
    "        X[i][4] = x5[i]\n",
    "    \n",
    "    Y = np.zeros((len(x1), 1))\n",
    "    for i in range(len(Y)):\n",
    "        Y[i] = y[i]\n",
    "\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "class Network:\n",
    "    def __init__(self, layers_, activation_functions, activation_grads_):\n",
    "        np.random.seed(42)\n",
    "        self.layers = layers_\n",
    "        self.W = [np.random.normal(0, 0.2, (layers_[i+1], layers_[i])) for i in range(len(layers_)-1)]\n",
    "        self.B = [np.random.normal(0, 0.2, (layers_[i], 1)) for i in range(1, len(layers_))]\n",
    "        self.activations = activation_functions\n",
    "        self.activation_grads = activation_grads_\n",
    "        \n",
    "\n",
    "    def evaluate(self, X):\n",
    "        values_a = [np.zeros((self.layers[i], 1)) for i in range(len(self.layers))]\n",
    "        values_z = [np.zeros((self.layers[i], 1)) for i in range(len(self.layers))]\n",
    "\n",
    "        for layer in range(len(values_z)):\n",
    "            if layer == 0:\n",
    "                for neuron in range(len(values_z[layer])):\n",
    "                    values_z[layer][neuron] = X[neuron]\n",
    "            else:\n",
    "                values_a[layer] = self.W[layer-1] @ values_z[layer-1] + self.B[layer-1]\n",
    "                values_z[layer] = self.activations[layer-1](np.copy(values_a[layer]))\n",
    "        return values_a, values_z\n",
    "\n",
    "    def backpropagation(self, values_a, values_z, Y, loss_grad):\n",
    "        W_grad = [np.zeros((len(self.W[i]), len(self.W[i][0]))) for i in range(len(self.W))]\n",
    "        B_grad = [np.zeros((len(self.B[i]), 1)) for i in range(len(self.B))]\n",
    "        delta = [np.zeros((len(values_z[i]), 1)) for i in range(len(values_z))]\n",
    "        err = loss_grad(Y, values_z[-1])  \n",
    "        delta[-1] = err @ self.activation_grads[-1](values_a[-1])\n",
    "        \n",
    "        W_grad[-1] = delta[-1] @ np.transpose(values_z[-2]) \n",
    "        B_grad[-1] = delta[-1]\n",
    "        for layer in range(len(self.layers)-1, 1, -1):\n",
    "            delta[layer-1] =  np.diag((self.activation_grads[layer-1](values_a[layer-1])).flatten()) @ np.transpose(self.W[layer-1]) @ delta[layer]\n",
    "            W_grad[layer - 2] = delta[layer-1] @ np.transpose(values_z[layer-2])\n",
    "            B_grad[layer-2] = delta[layer-1]\n",
    "        return W_grad, B_grad\n",
    "    \n",
    "    def gradient_descent(self, data_train, learning_rate, epochs): \n",
    "        for epoch in range(epochs):\n",
    "            W_grad_total = [np.zeros_like(w) for w in self.W]\n",
    "            B_grad_total = [np.zeros_like(b) for b in self.B]\n",
    "            for X, Y in data_train:\n",
    "                a, z  = self.evaluate(X)\n",
    "                W_grad, B_grad = self.backpropagation(a, z, Y, MSE_grad)\n",
    "                for i in range(len(W_grad_total)):\n",
    "                    W_grad_total[i] += W_grad[i]\n",
    "                    B_grad_total[i] += B_grad[i]\n",
    "            \n",
    "            for layer in range(len(self.W)):\n",
    "                self.W[layer] -= learning_rate * (W_grad_total[layer]/len(data_train))\n",
    "                self.B[layer] -= learning_rate * B_grad_total[layer]\n",
    "\n",
    "\n",
    "    def stochastic_gradient_descent(self, data_train, learning_rate, epochs):\n",
    "        for epoch in epochs:\n",
    "            for X, Y in data_train:\n",
    "                a, z  = self.evaluate(X)\n",
    "                W_grad, B_grad = self.backpropagation(a, z, Y, MSE_grad)\n",
    "                for layer in range(len(self.W)):\n",
    "                    self.W[layer] -= learning_rate * W_grad[layer]\n",
    "                    self.B[layer] -= learning_rate * B_grad[layer]\n",
    "\n",
    "\n",
    "    \n",
    "    def mini_baches(self, data_train, learning_rate, epochs, batch_size):\n",
    "        for epoch in epochs:\n",
    "            np.random.shuffle(data_train)\n",
    "            mini_batches = [data_train[k:k+batch_size] for k in range(0, len(data_train), batch_size)]\n",
    "            for mini_batch in mini_batches:\n",
    "                W_grad_total = [np.zeros_like(w) for w in self.W]\n",
    "                B_grad_total = [np.zeros_like(b) for b in self.B]\n",
    "                for X, Y in mini_batch:\n",
    "                    a, z  = self.evaluate(X)\n",
    "                    W_grad, B_grad = self.backpropagation(a, z, Y, MSE_grad)\n",
    "                    for i in range(len(W_grad_total)):\n",
    "                        W_grad_total[i] += W_grad[i]\n",
    "                        B_grad_total[i] += B_grad[i]\n",
    "                \n",
    "                for layer in range(len(self.W)):\n",
    "                    self.W[layer] -= learning_rate * W_grad_total[layer]\n",
    "                    self.B[layer] -= learning_rate * B_grad_total[layer]\n",
    "\n",
    "\n",
    "\n",
    "def relu(value):\n",
    "    for i in range(len(value)):\n",
    "        value[i] = max(0, value[i])\n",
    "    return  value\n",
    "\n",
    "def linear(x):\n",
    "    return x\n",
    "\n",
    "def MSE_grad(real_y, predict):\n",
    "    return 2*(predict - real_y)\n",
    "\n",
    "\n",
    "def relu_grad(x):\n",
    "    return np.where(x < 0, 0, 1)\n",
    "\n",
    "\n",
    "def linear_grad(x):\n",
    "    return np.ones_like(x)\n",
    "\n",
    "def error(data, predict):\n",
    "    return np.mean((data - predict)**2)\n",
    "\n",
    "def main():\n",
    "    path = 'datasets/Student_Performance_DEV.csv'\n",
    "    y_predict = []\n",
    "    data = pd.read_csv(path)\n",
    "    X, Y = dataset(data)\n",
    "    NN = Network([5, 2, 1], [relu, linear], [relu_grad, linear_grad])\n",
    "    data_train = [[X[i], Y[i]] for i in range(int(0.8*len(X)))]\n",
    "    NN.gradient_descent(data_train, 0.01, 3)\n",
    "\n",
    "\n",
    "\n",
    "    for i in range(int(0.8 * len(X)), len(X)):\n",
    "        y_predict.append(NN.evaluate(X[i])[1][-1][0])\n",
    "    err = error(Y[int(0.8 * len(X)):], y_predict)\n",
    "\n",
    "    print(\"error\", err)\n",
    "    \n",
    "        \n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
